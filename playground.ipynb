{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-2.3.2-cp38-cp38-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc==7.4.1\n",
      "  Downloading thinc-7.4.1-cp38-cp38-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
      "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl (31 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.2-cp38-cp38-macosx_10_9_x86_64.whl (19 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.2-cp38-cp38-macosx_10_9_x86_64.whl (183 kB)\n",
      "\u001b[K     |████████████████████████████████| 183 kB 50.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.2-cp38-cp38-macosx_10_9_x86_64.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 41.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.47.0)\n",
      "Requirement already satisfied: setuptools in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.18.5)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp38-cp38-macosx_10_9_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 28.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Installing collected packages: plac, cymem, murmurhash, srsly, catalogue, wasabi, preshed, blis, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 wasabi-0.8.0\n",
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 579 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=d0c3d669dd22e06348de31c5f032b6b8608a9dfd465c121e1506297eb10aa1db\n",
      "  Stored in directory: /private/var/folders/8p/z1z681ws78g80mhjsdwz6r9w0000gn/T/pip-ephem-wheel-cache-qe7sw62t/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages/en_core_web_sm -->\n",
      "/Users/somayoshida/opt/anaconda3/lib/python3.8/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When learning data science, you shouldn't get discouraged!\n",
      "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\n",
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '!', '\\n', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "my_doc = nlp(text)\n",
    "print(my_doc)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline 'sentencizer' component\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# create list of sentence tokens\n",
    "sents_list = []\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['others', 'you', 'hereupon', 'full', 'moreover', 'other', 'above', 'none', 'your', 'become']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "\n",
    "#Printing first ten stop words:\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: [learning, data, science, ,, discouraged, !, \n",
      ", Challenges, setbacks, failures, ,, journey, ., got, !]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#Implementation of stop words:\n",
    "filtered_sent=[]\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# filtering stop words\n",
    "for word in doc:\n",
    "    if word.is_stop==False:\n",
    "        filtered_sent.append(word)\n",
    "print(\"Filtered Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run run\n",
      "runs runs\n",
      "running running\n",
      "runner runner\n"
     ]
    }
   ],
   "source": [
    "# Implementing lemmatization\n",
    "lem = nlp(\"run runs running runner\")\n",
    "# finding lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text,word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on English in module spacy.lang.en object:\n",
      "\n",
      "class English(spacy.language.Language)\n",
      " |  English(vocab=True, make_doc=True, max_length=1000000, meta={}, **kwargs)\n",
      " |  \n",
      " |  A text-processing pipeline. Usually you'll load this once per process,\n",
      " |  and pass the instance around your application.\n",
      " |  \n",
      " |  Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
      " |      object and processing pipeline.\n",
      " |  lang (unicode): Two-letter language ID, i.e. ISO code.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/language\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      English\n",
      " |      spacy.language.Language\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Defaults = <class 'spacy.lang.en.EnglishDefaults'>\n",
      " |  \n",
      " |  \n",
      " |  lang = 'en'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __call__(self, text, disable=[], component_cfg=None)\n",
      " |      Apply the pipeline to some text. The text can span multiple sentences,\n",
      " |      and can contain arbitrary whitespace. Alignment into the original string\n",
      " |      is preserved.\n",
      " |      \n",
      " |      text (unicode): The text to be processed.\n",
      " |      disable (list): Names of the pipeline components to disable.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword arguments\n",
      " |          for specific components.\n",
      " |      RETURNS (Doc): A container for accessing the annotations.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#call\n",
      " |  \n",
      " |  __init__(self, vocab=True, make_doc=True, max_length=1000000, meta={}, **kwargs)\n",
      " |      Initialise a Language object.\n",
      " |      \n",
      " |      vocab (Vocab): A `Vocab` object. If `True`, a vocab is created via\n",
      " |          `Language.Defaults.create_vocab`.\n",
      " |      make_doc (callable): A function that takes text and returns a `Doc`\n",
      " |          object. Usually a `Tokenizer`.\n",
      " |      meta (dict): Custom meta data for the Language class. Is written to by\n",
      " |          models to add model meta data.\n",
      " |      max_length (int) :\n",
      " |          Maximum number of characters in a single text. The current v2 models\n",
      " |          may run out memory on extremely long texts, due to large internal\n",
      " |          allocations. You should segment these texts into meaningful units,\n",
      " |          e.g. paragraphs, subsections etc, before passing them to spaCy.\n",
      " |          Default maximum length is 1,000,000 characters (1mb). As a rule of\n",
      " |          thumb, if all pipeline components are enabled, spaCy's default\n",
      " |          models currently requires roughly 1GB of temporary memory per\n",
      " |          100,000 characters in one text.\n",
      " |      RETURNS (Language): The newly constructed object.\n",
      " |  \n",
      " |  add_pipe(self, component, name=None, before=None, after=None, first=None, last=None)\n",
      " |      Add a component to the processing pipeline. Valid components are\n",
      " |      callables that take a `Doc` object, modify it and return it. Only one\n",
      " |      of before/after/first/last can be set. Default behaviour is \"last\".\n",
      " |      \n",
      " |      component (callable): The pipeline component.\n",
      " |      name (unicode): Name of pipeline component. Overwrites existing\n",
      " |          component.name attribute if available. If no name is set and\n",
      " |          the component exposes no name attribute, component.__name__ is\n",
      " |          used. An error is raised if a name already exists in the pipeline.\n",
      " |      before (unicode): Component name to insert component directly before.\n",
      " |      after (unicode): Component name to insert component directly after.\n",
      " |      first (bool): Insert component first / not first in the pipeline.\n",
      " |      last (bool): Insert component last / not last in the pipeline.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#add_pipe\n",
      " |  \n",
      " |  begin_training(self, get_gold_tuples=None, sgd=None, component_cfg=None, **cfg)\n",
      " |      Allocate models, pre-process training data and acquire a trainer and\n",
      " |      optimizer. Used as a contextmanager.\n",
      " |      \n",
      " |      get_gold_tuples (function): Function returning gold data\n",
      " |      component_cfg (dict): Config parameters for specific components.\n",
      " |      **cfg: Config parameters.\n",
      " |      RETURNS: An optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#begin_training\n",
      " |  \n",
      " |  create_pipe(self, name, config={})\n",
      " |      Create a pipeline component from a factory.\n",
      " |      \n",
      " |      name (unicode): Factory name to look up in `Language.factories`.\n",
      " |      config (dict): Configuration parameters to initialise component.\n",
      " |      RETURNS (callable): Pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#create_pipe\n",
      " |  \n",
      " |  disable_pipes(self, *names)\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#disable_pipes\n",
      " |  \n",
      " |  evaluate(self, docs_golds, verbose=False, batch_size=256, scorer=None, component_cfg=None)\n",
      " |      Evaluate a model's pipeline components.\n",
      " |      \n",
      " |      docs_golds (iterable): Tuples of `Doc` and `GoldParse` objects.\n",
      " |      verbose (bool): Print debugging information.\n",
      " |      batch_size (int): Batch size to use.\n",
      " |      scorer (Scorer): Optional `Scorer` to use. If not passed in, a new one\n",
      " |          will be created.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      RETURNS (Scorer): The scorer containing the evaluation results.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#evaluate\n",
      " |  \n",
      " |  from_bytes(self, bytes_data, exclude=(), disable=None, **kwargs)\n",
      " |      Load state from a binary string.\n",
      " |      \n",
      " |      bytes_data (bytes): The data to load from.\n",
      " |      exclude (list): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_bytes\n",
      " |  \n",
      " |  from_disk(self, path, exclude=(), disable=None)\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it. If the saved `Language` object contains a model, the\n",
      " |      model will be loaded.\n",
      " |      \n",
      " |      path (unicode or Path): A path to a directory.\n",
      " |      exclude (list): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The modified `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_disk\n",
      " |  \n",
      " |  get_pipe(self, name)\n",
      " |      Get a pipeline component for a given component name.\n",
      " |      \n",
      " |      name (unicode): Name of pipeline component to get.\n",
      " |      RETURNS (callable): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#get_pipe\n",
      " |  \n",
      " |  has_pipe(self, name)\n",
      " |      Check if a component name is present in the pipeline. Equivalent to\n",
      " |      `name in nlp.pipe_names`.\n",
      " |      \n",
      " |      name (unicode): Name of the component.\n",
      " |      RETURNS (bool): Whether a component of the name exists in the pipeline.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#has_pipe\n",
      " |  \n",
      " |  make_doc(self, text)\n",
      " |  \n",
      " |  pipe(self, texts, as_tuples=False, n_threads=-1, batch_size=1000, disable=[], cleanup=False, component_cfg=None, n_process=1)\n",
      " |      Process texts as a stream, and yield `Doc` objects in order.\n",
      " |      \n",
      " |      texts (iterator): A sequence of texts to process.\n",
      " |      as_tuples (bool): If set to True, inputs should be a sequence of\n",
      " |          (text, context) tuples. Output will then be a sequence of\n",
      " |          (doc, context) tuples. Defaults to False.\n",
      " |      batch_size (int): The number of texts to buffer.\n",
      " |      disable (list): Names of the pipeline components to disable.\n",
      " |      cleanup (bool): If True, unneeded strings are freed to control memory\n",
      " |          use. Experimental.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      n_process (int): Number of processors to process texts, only supported\n",
      " |          in Python3. If -1, set `multiprocessing.cpu_count()`.\n",
      " |      YIELDS (Doc): Documents in the order of the original text.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#pipe\n",
      " |  \n",
      " |  preprocess_gold(self, docs_golds)\n",
      " |      Can be called before training to pre-process gold data. By default,\n",
      " |      it handles nonprojectivity and adds missing tags to the tag map.\n",
      " |      \n",
      " |      docs_golds (iterable): Tuples of `Doc` and `GoldParse` objects.\n",
      " |      YIELDS (tuple): Tuples of preprocessed `Doc` and `GoldParse` objects.\n",
      " |  \n",
      " |  rehearse(self, docs, sgd=None, losses=None, config=None)\n",
      " |      Make a \"rehearsal\" update to the models in the pipeline, to prevent\n",
      " |      forgetting. Rehearsal updates run an initial copy of the model over some\n",
      " |      data, and update the model so its current predictions are more like the\n",
      " |      initial ones. This is useful for keeping a pretrained model on-track,\n",
      " |      even if you're updating it with a smaller set of examples.\n",
      " |      \n",
      " |      docs (iterable): A batch of `Doc` objects.\n",
      " |      drop (float): The dropout rate.\n",
      " |      sgd (callable): An optimizer.\n",
      " |      RETURNS (dict): Results from the update.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> raw_text_batches = minibatch(raw_texts)\n",
      " |          >>> for labelled_batch in minibatch(zip(train_docs, train_golds)):\n",
      " |          >>>     docs, golds = zip(*train_docs)\n",
      " |          >>>     nlp.update(docs, golds)\n",
      " |          >>>     raw_batch = [nlp.make_doc(text) for text in next(raw_text_batches)]\n",
      " |          >>>     nlp.rehearse(raw_batch)\n",
      " |  \n",
      " |  remove_pipe(self, name)\n",
      " |      Remove a component from the pipeline.\n",
      " |      \n",
      " |      name (unicode): Name of the component to remove.\n",
      " |      RETURNS (tuple): A `(name, component)` tuple of the removed component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#remove_pipe\n",
      " |  \n",
      " |  rename_pipe(self, old_name, new_name)\n",
      " |      Rename a pipeline component.\n",
      " |      \n",
      " |      old_name (unicode): Name of the component to rename.\n",
      " |      new_name (unicode): New name of the component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rename_pipe\n",
      " |  \n",
      " |  replace_pipe(self, name, component)\n",
      " |      Replace a component in the pipeline.\n",
      " |      \n",
      " |      name (unicode): Name of the component to replace.\n",
      " |      component (callable): Pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_pipe\n",
      " |  \n",
      " |  resume_training(self, sgd=None, **cfg)\n",
      " |      Continue training a pretrained model.\n",
      " |      \n",
      " |      Create and return an optimizer, and initialize \"rehearsal\" for any pipeline\n",
      " |      component that has a .rehearse() method. Rehearsal is used to prevent\n",
      " |      models from \"forgetting\" their initialised \"knowledge\". To perform\n",
      " |      rehearsal, collect samples of text you want the models to retain performance\n",
      " |      on, and call nlp.rehearse() with a batch of Doc objects.\n",
      " |  \n",
      " |  to_bytes(self, exclude=(), disable=None, **kwargs)\n",
      " |      Serialize the current state to a binary string.\n",
      " |      \n",
      " |      exclude (list): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (bytes): The serialized form of the `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_bytes\n",
      " |  \n",
      " |  to_disk(self, path, exclude=(), disable=None)\n",
      " |      Save the current state to a directory.  If a model is loaded, this\n",
      " |      will include the model.\n",
      " |      \n",
      " |      path (unicode or Path): Path to a directory, which will be created if\n",
      " |          it doesn't exist.\n",
      " |      exclude (list): Names of components or serialization fields to exclude.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_disk\n",
      " |  \n",
      " |  update(self, docs, golds, drop=0.0, sgd=None, losses=None, component_cfg=None)\n",
      " |      Update the models in the pipeline.\n",
      " |      \n",
      " |      docs (iterable): A batch of `Doc` objects.\n",
      " |      golds (iterable): A batch of `GoldParse` objects.\n",
      " |      drop (float): The dropout rate.\n",
      " |      sgd (callable): An optimizer.\n",
      " |      losses (dict): Dictionary to update with the loss, keyed by component.\n",
      " |      component_cfg (dict): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#update\n",
      " |  \n",
      " |  use_params(self, params, **cfg)\n",
      " |      Replace weights of models in the pipeline with those provided in the\n",
      " |      params dictionary. Can be used as a contextmanager, in which case,\n",
      " |      models go back to their original weights after the block.\n",
      " |      \n",
      " |      params (dict): A dictionary of parameters keyed by model ID.\n",
      " |      **cfg: Config parameters.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> with nlp.use_params(optimizer.averages):\n",
      " |          >>>     nlp.to_disk('/tmp/checkpoint')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from spacy.language.Language:\n",
      " |  \n",
      " |  entity\n",
      " |  \n",
      " |  linker\n",
      " |  \n",
      " |  matcher\n",
      " |  \n",
      " |  parser\n",
      " |  \n",
      " |  path\n",
      " |  \n",
      " |  pipe_factories\n",
      " |      Get the component factories for the available pipeline components.\n",
      " |      \n",
      " |      RETURNS (dict): Factory names, keyed by component names.\n",
      " |  \n",
      " |  pipe_labels\n",
      " |      Get the labels set by the pipeline components, if available (if\n",
      " |      the component exposes a labels property).\n",
      " |      \n",
      " |      RETURNS (dict): Labels keyed by component name.\n",
      " |  \n",
      " |  pipe_names\n",
      " |      Get names of available pipeline components.\n",
      " |      \n",
      " |      RETURNS (list): List of component name strings, in order.\n",
      " |  \n",
      " |  tagger\n",
      " |  \n",
      " |  tensorizer\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  meta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from spacy.language.Language:\n",
      " |  \n",
      " |  factories = {'entity_linker': <function component.__call__.<locals>.fa...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DET\n",
      "is AUX\n",
      "well ADV\n",
      "that DET\n",
      "ends VERB\n",
      "well ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "\n",
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm\n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "#  \"nlp\" Objectis used to create documents with linguistic annotations.\n",
    "docs = nlp(u\"All is well that ends well.\")\n",
    "\n",
    "for word in docs:\n",
    "    print(word.text,word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_tweets = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "Python 15\n",
      "developer 22\n",
      "currently 32\n",
      "working 42\n",
      "for 50\n",
      "a 54\n",
      "London 56\n",
      "- 62\n",
      "based 63\n",
      "Fintech 69\n",
      "company 77\n",
      ". 84\n",
      "He 86\n",
      "is 89\n",
      "interested 92\n",
      "in 103\n",
      "learning 106\n",
      "Natural 115\n",
      "Language 123\n",
      "Processing 132\n",
      ". 142\n"
     ]
    }
   ],
   "source": [
    "about_text = ('Gus Proto is a Python developer currently'\n",
    "               ' working for a London-based Fintech'\n",
    "               ' company. He is interested in learning'\n",
    "               ' Natural Language Processing.')\n",
    "nlp_doc = nlp(about_text)\n",
    "for token in nlp_doc:\n",
    "    print(token, token.idx)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Gus NNP\n",
      "is be VBZ\n",
      "helping help VBG\n",
      "organize organize VB\n",
      "a a DT\n",
      "developerconference developerconference NN\n",
      "on on IN\n",
      "Applications Applications NNPS\n",
      "of of IN\n",
      "Natural Natural NNP\n",
      "Language Language NNP\n",
      "Processing Processing NNP\n",
      ". . .\n",
      "He -PRON- PRP\n",
      "keeps keep VBZ\n",
      "organizing organize VBG\n",
      "local local JJ\n",
      "Python Python NNP\n",
      "meetups meetup NNS\n",
      "and and CC\n",
      "several several JJ\n",
      "internal internal JJ\n",
      "talks talk NNS\n",
      "at at IN\n",
      "his -PRON- PRP$\n",
      "workplace workplace NN\n",
      ". . .\n"
     ]
    }
   ],
   "source": [
    "conference_help_text = ('Gus is helping organize a developer'\n",
    "     'conference on Applications of Natural Language'\n",
    "     ' Processing. He keeps organizing local Python meetups'\n",
    "     ' and several internal talks at his workplace.')\n",
    "conference_help_doc = nlp(conference_help_text)\n",
    "for token in conference_help_doc:\n",
    "    print (token, token.lemma_,token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "# displacy.serve(conference_help_doc, style='dep')\n",
    "displacy.render(conference_help_doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "link = 'https://t.co/smyYriipxI'\n",
    "link_pattern = r'http[s]?://[\\w|.|/]+'\n",
    "x = re.sub(link_pattern, 'success', link)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 10), match='@PKuchly57'>\n"
     ]
    }
   ],
   "source": [
    "mention = '@PKuchly57'\n",
    "mention_pattern = r'@\\w+'\n",
    "x = re.match(mention_pattern, mention)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='#'>\n"
     ]
    }
   ],
   "source": [
    "mention = '#'\n",
    "mention_pattern = r'(@\\w+)|(#)'\n",
    "x = re.match(mention_pattern, mention)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 8663801465970268676 #\n",
      "FollowFriday 4558212729026417302 FollowFriday\n",
      "@France_Inte 15376876697887500918 @france_inte\n",
      "@PKuchly57 12282724241219763148 @PKuchly57\n",
      "@Milipol_Paris 588482831550294546 @milipol_paris\n",
      "for 16037325823156266367 for\n",
      "being 10382539506755952630 be\n",
      "top 8328343100126676325 top\n",
      "engaged 12307195711836261817 engaged\n",
      "members 14721843519903598875 member\n",
      "in 3002984154512732771 in\n",
      "my 561228191312463089 -PRON-\n",
      "community 17822516981717808594 community\n",
      "this 1995909169258310477 this\n",
      "week 14249255431398666181 week\n",
      ":) 5920004935509210957 :)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp('#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)')\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 8663801465970268676 #\n",
      "FollowFriday 4558212729026417302 FollowFriday\n",
      "@France_Inte 16733713287512299168 @France_Inte\n",
      "@PKuchly57 12282724241219763148 @PKuchly57\n",
      "@Milipol_Paris 9110577758730473699 @Milipol_Paris\n",
      "for 16037325823156266367 for\n",
      "being 3899131925553995529 being\n",
      "top 8328343100126676325 top\n",
      "engaged 12307195711836261817 engaged\n",
      "members 1000530315840773259 members\n",
      "in 3002984154512732771 in\n",
      "my 227504873216781231 my\n",
      "community 17822516981717808594 community\n",
      "this 1995909169258310477 this\n",
      "week 14249255431398666181 week\n",
      ":) 5920004935509210957 :)\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "doc = tokenizer('#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)')\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 8663801465970268676 #\n",
      "FollowFriday 4558212729026417302 FollowFriday\n",
      "@France_Inte 16733713287512299168 @France_Inte\n",
      "@PKuchly57 12282724241219763148 @PKuchly57\n",
      "@Milipol_Paris 9110577758730473699 @Milipol_Paris\n",
      "for 16037325823156266367 for\n",
      "being 3899131925553995529 being\n",
      "top 8328343100126676325 top\n",
      "engaged 12307195711836261817 engaged\n",
      "members 1000530315840773259 members\n",
      "in 3002984154512732771 in\n",
      "my 227504873216781231 my\n",
      "community 17822516981717808594 community\n",
      "this 1995909169258310477 this\n",
      "week 14249255431398666181 week\n",
      ":) 5920004935509210957 :)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "doc = nlp('#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)')\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text = '123455566'\n",
    "nums_pattern = r'^[0-9]+$'\n",
    "if re.match(nums_pattern, text):\n",
    "    print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Lab\\\\nltk_data\\\\corpora\\\\movie_reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8e16b1ba34aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# loading all files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmovie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoviedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/datasets/_base.py\u001b[0m in \u001b[0;36mload_files\u001b[0;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     folders = [f for f in sorted(listdir(container_path))\n\u001b[0m\u001b[1;32m    187\u001b[0m                if isdir(join(container_path, f))]\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Lab\\\\nltk_data\\\\corpora\\\\movie_reviews'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "moviedir = r'/Users/somayoshida/nltk_data/corpora/movie_reviews'\n",
    "\n",
    "# loading all files. \n",
    "movie = load_files(moviedir, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(movie.data, movie.target, \n",
    "                                                          test_size = 0.20, random_state = 12)\n",
    "\n",
    "\n",
    "# initialize CountVectorizer\n",
    "movieVzer= CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=3000) # use top 3000 words only. 78.25% acc.\n",
    "# movieVzer = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize)         # use all 25K words. Higher accuracy\n",
    "\n",
    "# fit and tranform using training text \n",
    "docs_train_counts = movieVzer.fit_transform(docs_train)\n",
    "\n",
    "\n",
    "# Convert raw frequency counts into TF-IDF values\n",
    "movieTfmer = TfidfTransformer()\n",
    "docs_train_tfidf = movieTfmer.fit_transform(docs_train_counts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# testing data\n",
    "\n",
    "# Using the fitted vectorizer and transformer, tranform the test data\n",
    "docs_test_counts = movieVzer.transform(docs_test)\n",
    "docs_test_tfidf = movieTfmer.transform(docs_test_counts)\n",
    "\n",
    "\n",
    "\n",
    "# Now ready to build a classifier. \n",
    "# We will use Multinominal Naive Bayes as our model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Train a Multimoda Naive Bayes classifier. Again, we call it \"fitting\"\n",
    "clf = MultinomialNB()\n",
    "clf.fit(docs_train_tfidf, y_train)\n",
    "\n",
    "# Predict the Test set results, find accuracy\n",
    "y_pred = clf.predict(docs_test_tfidf)\n",
    "sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# trying the classifier\n",
    "# very short and fake movie reviews\n",
    "reviews_new = ['This movie was excellent', 'Absolute joy ride', \n",
    "            'Steven Seagal was terrible', 'Steven Seagal shone through.', \n",
    "              'This was certainly a movie', 'Two thumbs up', 'I fell asleep halfway through', \n",
    "              \"We can't wait for the sequel!!\", '!', '?', 'I cannot recommend this highly enough', \n",
    "              'instant classic.', 'Steven Seagal was amazing. His performance was Oscar-worthy.']\n",
    "\n",
    "reviews_new_counts = movieVzer.transform(reviews_new)         # turn text into count vector\n",
    "reviews_new_tfidf = movieTfmer.transform(reviews_new_counts)  # turn into tfidf vector\n",
    "\n",
    "\n",
    "# have classifier make a prediction\n",
    "pred = clf.predict(reviews_new_tfidf)\n",
    "\n",
    "# print out results\n",
    "for review, category in zip(reviews_new, pred):\n",
    "    print('%r => %s' % (review, movie.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
